{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"',\n",
    "                        shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74319e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import random\n",
    "from typing import Any, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "\n",
    "import constants\n",
    "import data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3353830b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\\n  <siteinfo>\\n    <sitename>Wikipedia</sitename>\\n    <base>http://en.wikipedia.org/wiki/Main_Page</base>\\n    <generator>MediaWiki 1.6alpha</generator>\\n    <case>first-letter</case>\\n      <namespaces>\\n      <namespace key=\"-2\">Media</namespace>\\n      <namespace key=\"-1\">Special</namespace>\\n      <namespace key=\"0\" />\\n      <namespace key=\"1\">Talk</namespace>\\n      <namespace key=\"2\">User</namespace>\\n      <namespace key=\"3\">User talk</namespace>\\n      <namespace key=\"4\">Wikipedia</namespace>\\n      <namespace key=\"5\">Wikipedia talk</namespace>\\n      <namespace key=\"6\">Image</namespace>\\n      <namespace key=\"7\">Image talk</namespace>\\n      <namespace key=\"8\">MediaWiki</namespace>\\n      <namespace key=\"9\">MediaWiki talk</namespace>\\n      <namespace key=\"10\">Template</namespace>\\n      <namespace key=\"11\">Template talk</namespace>\\n      <namespace key=\"12\">Help</namespace>\\n      <namespace key=\"13\">Help talk</namespace>\\n      <namespace key=\"14\">Category</namespace>\\n      <namespace key=\"15\">Category talk</namespace>\\n      <namespace key=\"100\">Portal</namespace>\\n      <namespace key=\"101\">Portal talk</namespace>\\n    </namespaces>\\n  </siteinfo>\\n  <page>\\n    <title>AaA</title>\\n    <id>1</id>\\n    <revision>\\n      <id>32899315</id>\\n      <timestamp>2005-12-27T18:46:47Z</timestamp>\\n      <contributor>\\n        <username>Jsmethers</username>\\n        <id>614213</id>\\n      </contributor>\\n      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\\n    </revision>\\n  </page>\\n  <page>\\n    <title>AlgeriA</title>\\n    <id>5</id>\\n    <revision>\\n      <id>18063769</id>\\n      <timestamp>2005-07-03T11:13:13Z</timestamp>\\n      <contributor>\\n        <username>Docu</username>\\n        <id>8029</id>\\n      </contributor>\\n      <minor />\\n      <comment>adding cur_id=5: {{R from CamelCase}}</comment>\\n    '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = constants.CHUNK_SIZE_BYTES\n",
    "\n",
    "# Dataset 與取 batch\n",
    "enwik8_data_generator = data_loaders.get_enwik9_iterator(\n",
    "    num_chunks=constants.NUM_CHUNKS // 10, # 只取了完整的 EnWik9 数据集的 10% 部分，也就是 EnWik8\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "\n",
    "next(enwik8_data_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "18c1df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enwik8Dataset(Dataset):\n",
    "    \"\"\"Dataset for Enwik data.\"\"\"\n",
    "    def __init__(self, data_chunks) -> None:\n",
    "        self.dataset = data_chunks\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        seq = self.dataset[idx]\n",
    "        seq_ascii = np.frombuffer(seq, dtype=np.uint8)\n",
    "        # 依然回傳 uint8；模型內會轉為 long\n",
    "        return torch.tensor(seq_ascii, dtype=torch.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "768509ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Config\n",
    "# ----------------------------\n",
    "class TransformerConfig:\n",
    "    \"\"\"Hyperparameters used in the Transformer architectures.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        embedding_dim: int = 64,\n",
    "        num_layers: int = 4,\n",
    "        num_heads: int = 8,\n",
    "        emb_init_scale: float = 0.02,\n",
    "        widening_factor: int = 4,\n",
    "        dropout: float = 0.0,\n",
    "        max_length: int = constants.CHUNK_SIZE_BYTES,\n",
    "        bos_token_id: int = 0,\n",
    "        tie_weights: bool = False,  # 如需與輸入 embedding 綁定，改成 True\n",
    "    ) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.emb_init_scale = emb_init_scale\n",
    "        self.widening_factor = widening_factor\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.tie_weights = tie_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b9edb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Sinusoidal Positional Encoding（PyTorch 沒內建，保留精簡版）\n",
    "# ----------------------------\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 4096):\n",
    "        super().__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)                # [T, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)                               # [T, D]\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe, persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [B, T, D] -> 回傳 [T, D]，之後會 broadcast 到 batch 維度\n",
    "        T = x.size(1)\n",
    "        return self.pe[:T]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ba305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 使用 PyTorch 內建 TransformerDecoder 的語言模型\n",
    "# ----------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"Transformer decoder model (PyTorch built-ins).\"\"\"\n",
    "\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.embedding_dim,\n",
    "        )\n",
    "        # 对神经网络中的 embedding layer 权重进行初始化\n",
    "        # 使用的是截断正态分布（truncated normal distribution）\n",
    "        nn.init.trunc_normal_(self.embedding.weight, std=config.emb_init_scale)\n",
    "\n",
    "        # Positional encoding（固定正弦）\n",
    "        self.pos_encoding = SinusoidalPositionalEncoding(\n",
    "            d_model=config.embedding_dim,\n",
    "            max_len=config.max_length,\n",
    "        )\n",
    "\n",
    "        # 內建 Decoder Layer + 堆疊\n",
    "        d_model = config.embedding_dim\n",
    "        dim_ff = d_model * config.widening_factor\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=config.num_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=config.dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,      # 讓張量是 [B, T, D]\n",
    "            norm_first=True,       # Pre-LN 比較穩定\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            decoder_layer=decoder_layer,\n",
    "            num_layers=config.num_layers,\n",
    "        )\n",
    "\n",
    "        # 最終 LayerNorm（常見做法，可留可去）\n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # 輸出線性層\n",
    "        self.output_layer = nn.Linear(d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # 可選：權重綁定（weight tying）\n",
    "        # 是否让 output 层的權重與 input embedding 層的權重綁定（一般默认是 fasle）\n",
    "        if config.tie_weights:\n",
    "            self.output_layer.weight = self.embedding.weight\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _causal_mask(self, T: int, device) -> torch.Tensor:\n",
    "        # PyTorch 的 attn_mask: True 表示不允許注意（被遮蔽）\n",
    "        return torch.triu(torch.ones(T, T, device=device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "    def shift_right(self, sequences: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Right-shift the input by padding on the temporal axis.\"\"\"\n",
    "        sequences = sequences.long()\n",
    "        bos = torch.full(\n",
    "            (sequences.size(0), 1),\n",
    "            fill_value=self.config.bos_token_id,\n",
    "            dtype=sequences.dtype,\n",
    "            device=sequences.device,\n",
    "        )\n",
    "        return torch.cat([bos, sequences[:, :-1]], dim=1)\n",
    "\n",
    "    def forward(self, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            log_probs: [B, T, V]（與原程式一致：回傳 log_softmax，供 to_marginals 使用）\n",
    "        \"\"\"\n",
    "        # 右移得到自迴歸輸入\n",
    "        inputs = self.shift_right(targets)                   # [B, T] long\n",
    "\n",
    "        # Token + Positional\n",
    "        x = self.embedding(inputs)                           # [B, T, D]\n",
    "        x = x * math.sqrt(self.config.embedding_dim)\n",
    "        pos = self.pos_encoding(x).to(x.device)              # [T, D]\n",
    "        x = x + pos                                          # broadcast 到 [B, T, D]\n",
    "\n",
    "        # 因果遮罩（純 LM 無 encoder memory）\n",
    "        T = x.size(1)\n",
    "        attn_mask = self._causal_mask(T, x.device)           # [T, T] bool\n",
    "\n",
    "        # Decoder forward（不需要 memory）\n",
    "        h = self.decoder(tgt=x, memory=x, tgt_mask=attn_mask)\n",
    "        h = self.final_norm(h)\n",
    "\n",
    "        logits = self.output_layer(h)                        # [B, T, V]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9c2465c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Tuple\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SentenceLevelNLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentenceLevelNLoss, self).__init__()\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        true_predictions = torch.gather(log_probs, 2, targets.long().unsqueeze(2)).squeeze(2)\n",
    "        sentence_loss = -torch.mean(torch.sum(true_predictions, dim=1))\n",
    "        return sentence_loss\n",
    "\n",
    "def train_transformer_decoder(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    training_steps: int,\n",
    "    log_every: int,\n",
    "    use_tqdm: bool = True,\n",
    "    device: str = 'cuda',\n",
    ") -> Tuple[nn.Module, float]:\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = SentenceLevelNLoss()\n",
    "    \n",
    "    print('Initialization done, starting training...')\n",
    "    last_loss = 0.0\n",
    "    data_iter = itertools.cycle(data_loader)\n",
    "    for step in tqdm(range(training_steps), disable=not use_tqdm):\n",
    "        batch = next(data_iter).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch)                  # [B, T, V] logits\n",
    "        loss = loss_fn(logits, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if log_every > 0 and step % log_every == 0:\n",
    "            print(f'Step {step}, Loss {loss.item()}')\n",
    "\n",
    "        last_loss = loss.item()\n",
    "\n",
    "    return model, last_loss\n",
    "\n",
    "def train_transformer_decoder_by_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    log_every: int,\n",
    "    use_tqdm: bool = True,\n",
    "    device: str = 'cuda',\n",
    ") -> Tuple[nn.Module, float]:\n",
    "    \"\"\"\n",
    "    按轮次训练Transformer解码器的函数。\n",
    "\n",
    "    参数:\n",
    "    model (nn.Module): 要训练的模型\n",
    "    data_loader (DataLoader): 数据加载器\n",
    "    num_epochs (int): 训练的轮数\n",
    "    log_every (int): 每隔多少步打印一次日志\n",
    "    use_tqdm (bool): 是否使用tqdm显示进度条，默认为True\n",
    "    device (str): 使用的设备，默认为'cuda'\n",
    "\n",
    "    返回:\n",
    "    Tuple[nn.Module, float]: 训练后的模型和最后一轮的最后一个损失值\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # 优化器\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    loss_fn = SentenceLevelNLoss()\n",
    "    \n",
    "    print('Initialization done, starting training...')\n",
    "    last_loss = 0.0\n",
    "    for epoch in tqdm(range(num_epochs), disable=not use_tqdm):\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(batch)                  # [B, T, V] logits\n",
    "            loss = loss_fn(logits, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if log_every > 0 and step % log_every == 0:\n",
    "                print(f'Epoch {epoch}, Step {step}, Loss {loss.item()}')\n",
    "\n",
    "            last_loss = loss.item()\n",
    "\n",
    "    return model, last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5fbc97ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vocab_size': 256, 'embedding_dim': 64, 'num_layers': 4, 'num_heads': 8, 'emb_init_scale': 0.02, 'widening_factor': 4, 'dropout': 0.0, 'max_length': 2048, 'bos_token_id': 0, 'tie_weights': False}\n",
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "TransformerDecoder                            [2, 2048, 256]            --\n",
      "├─Embedding: 1-1                              [2, 2048, 64]             16,384\n",
      "├─SinusoidalPositionalEncoding: 1-2           [2048, 64]                --\n",
      "├─TransformerDecoder: 1-3                     [2, 2048, 64]             --\n",
      "│    └─ModuleList: 2-1                        --                        --\n",
      "│    │    └─TransformerDecoderLayer: 3-1      [2, 2048, 64]             66,752\n",
      "│    │    └─TransformerDecoderLayer: 3-2      [2, 2048, 64]             66,752\n",
      "│    │    └─TransformerDecoderLayer: 3-3      [2, 2048, 64]             66,752\n",
      "│    │    └─TransformerDecoderLayer: 3-4      [2, 2048, 64]             66,752\n",
      "├─LayerNorm: 1-4                              [2, 2048, 64]             128\n",
      "├─Linear: 1-5                                 [2, 2048, 256]            16,384\n",
      "===============================================================================================\n",
      "Total params: 299,904\n",
      "Trainable params: 299,904\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 0.33\n",
      "===============================================================================================\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 79.69\n",
      "Params size (MB): 0.67\n",
      "Estimated Total Size (MB): 80.39\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create config and model\n",
    "config = TransformerConfig(vocab_size=constants.ALPHABET_SIZE)\n",
    "print(config.__dict__)\n",
    "model = TransformerDecoder(config)\n",
    "print(summary(model, input_size=(2, constants.CHUNK_SIZE_BYTES), dtypes=[torch.long]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70329952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 2048]) torch.uint8\n",
      "1526\n"
     ]
    }
   ],
   "source": [
    "sequence_length = constants.CHUNK_SIZE_BYTES\n",
    "\n",
    "# Dataset 與取 batch\n",
    "enwik8_data_generator = data_loaders.get_enwik9_iterator(\n",
    "    num_chunks=constants.NUM_CHUNKS // 10, # 只取了完整的 EnWik9 数据集的 10% 部分，也就是 EnWik8\n",
    "    sequence_length=sequence_length,\n",
    ")\n",
    "enwik8_chunks = list(enwik8_data_generator)\n",
    "enwik8Dataset = Enwik8Dataset(enwik8_chunks)\n",
    "enwik8DataLoader = DataLoader(enwik8Dataset, batch_size=32, shuffle=True)\n",
    "for batch in enwik8DataLoader:\n",
    "    print(batch.shape, batch.dtype)\n",
    "    break\n",
    "print(len(enwik8DataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3d08dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization done, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss 11639.6328125\n",
      "Epoch 0, Step 500, Loss 6236.7177734375\n",
      "Epoch 0, Step 1000, Loss 5742.93408203125\n",
      "Epoch 0, Step 1500, Loss 5561.9580078125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [07:42<15:24, 462.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 0, Loss 5458.1064453125\n",
      "Epoch 1, Step 500, Loss 5413.98046875\n",
      "Epoch 1, Step 1000, Loss 5168.072265625\n",
      "Epoch 1, Step 1500, Loss 5097.517578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [15:32<07:47, 467.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 0, Loss 5192.1435546875\n",
      "Epoch 2, Step 500, Loss 5176.4443359375\n",
      "Epoch 2, Step 1000, Loss 5035.2255859375\n",
      "Epoch 2, Step 1500, Loss 5007.82763671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [22:58<00:00, 459.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 4857.38671875\n",
      "Parameters saved in file params.pth\n"
     ]
    }
   ],
   "source": [
    "model, loss = train_transformer_decoder_by_epoch(\n",
    "    model=model,\n",
    "    data_loader=enwik8DataLoader,\n",
    "    num_epochs=3,\n",
    "    log_every=500,\n",
    "    device=device\n",
    ")\n",
    "print(f'Final loss: {loss}')\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'params.pth')\n",
    "print('Parameters saved in file params.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "495bee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data sample size: 2048 bytes\n",
      "<class 'numpy.ndarray'> [ 60 109 101 ...  32  32  32]\n",
      "b'<mediawiki xmlns=\"http://www.mediawiki.org/xml/export-0.3/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.3/ http://www.mediawiki.org/xml/export-0.3.xsd\" version=\"0.3\" xml:lang=\"en\">\\n  <siteinfo>\\n    <sitename>Wikipedia</sitename>\\n    <base>http://en.wikipedia.org/wiki/Main_Page</base>\\n    <generator>MediaWiki 1.6alpha</generator>\\n    <case>first-letter</case>\\n      <namespaces>\\n      <namespace key=\"-2\">Media</namespace>\\n      <namespace key=\"-1\">Special</namespace>\\n      <namespace key=\"0\" />\\n      <namespace key=\"1\">Talk</namespace>\\n      <namespace key=\"2\">User</namespace>\\n      <namespace key=\"3\">User talk</namespace>\\n      <namespace key=\"4\">Wikipedia</namespace>\\n      <namespace key=\"5\">Wikipedia talk</namespace>\\n      <namespace key=\"6\">Image</namespace>\\n      <namespace key=\"7\">Image talk</namespace>\\n      <namespace key=\"8\">MediaWiki</namespace>\\n      <namespace key=\"9\">MediaWiki talk</namespace>\\n      <namespace key=\"10\">Template</namespace>\\n      <namespace key=\"11\">Template talk</namespace>\\n      <namespace key=\"12\">Help</namespace>\\n      <namespace key=\"13\">Help talk</namespace>\\n      <namespace key=\"14\">Category</namespace>\\n      <namespace key=\"15\">Category talk</namespace>\\n      <namespace key=\"100\">Portal</namespace>\\n      <namespace key=\"101\">Portal talk</namespace>\\n    </namespaces>\\n  </siteinfo>\\n  <page>\\n    <title>AaA</title>\\n    <id>1</id>\\n    <revision>\\n      <id>32899315</id>\\n      <timestamp>2005-12-27T18:46:47Z</timestamp>\\n      <contributor>\\n        <username>Jsmethers</username>\\n        <id>614213</id>\\n      </contributor>\\n      <text xml:space=\"preserve\">#REDIRECT [[AAA]]</text>\\n    </revision>\\n  </page>\\n  <page>\\n    <title>AlgeriA</title>\\n    <id>5</id>\\n    <revision>\\n      <id>18063769</id>\\n      <timestamp>2005-07-03T11:13:13Z</timestamp>\\n      <contributor>\\n        <username>Docu</username>\\n        <id>8029</id>\\n      </contributor>\\n      <minor />\\n      <comment>adding cur_id=5: {{R from CamelCase}}</comment>\\n    '\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from language_modeling_is_compression import constants\n",
    "from language_modeling_is_compression import data_loaders\n",
    "\n",
    "enwik9_data_generator = data_loaders.get_enwik9_iterator(\n",
    "      num_chunks=10,\n",
    "      chunk_start_idx= constants.NUM_CHUNKS // 10,\n",
    "      sequence_length=constants.CHUNK_SIZE_BYTES,\n",
    ")\n",
    "\n",
    "rawdata = next(enwik9_data_generator)\n",
    "print(f'Data sample size: {len(rawdata)} bytes')\n",
    "\n",
    "tokenized_data = np.frombuffer(rawdata, dtype=np.uint8)\n",
    "print(type(tokenized_data), tokenized_data)\n",
    "print(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "930bc59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(tokenized_data_batch):\n",
    "    config = TransformerConfig(vocab_size=constants.ALPHABET_SIZE)\n",
    "    model = TransformerDecoder(config)\n",
    "    params = torch.load('params.pth')\n",
    "    model.load_state_dict(params)\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        x = torch.tensor(tokenized_data_batch, dtype=torch.int64)\n",
    "        # Get logits from the model\n",
    "        logits = model(x)\n",
    "        # Convert to log probabilities and then to numpy\n",
    "        log_probs = torch.log_softmax(logits, dim=-1).cpu().numpy()\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4100958",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_slow_lossless_compression = False\n",
    "if use_slow_lossless_compression:\n",
    "  # use_slow_lossless_compression:\n",
    "  log_probs = list()\n",
    "  for t in range(len(tokenized_data)):\n",
    "    # assume tokenized_data = [h,e,l,l,o]\n",
    "    # t0: input = [h] 實際模型的輸入是 [BOS]，因為還要做 right_shift,模型的輸出是 p(h | <bos>)\n",
    "    # t1: input = [h,e] 實際模型的輸入是 [h]，因為還要做 right_shift，模型的輸出是 p(e | h)\n",
    "    # t2: input = [h,e,l]\n",
    "    # t3: input = [h,e,l,l]\n",
    "    # t4: input = [h,e,l,l,o]\n",
    "    # 为什么这里必须一步步来，不能一次输入所有的 tokenized_data 呢，因为解码是一个一个token解码的。\n",
    "    # 直接一次 forward \"<bos> h e l l\" 在 hell位置上得到的 logits 其实跟一步步的logits不同\n",
    "    # 为什么不同？因为 casual mask 只是应用在了 attention 的计算里，而整个transformer 的计算组件里还有很多\n",
    "    # 没有用到 casual mask 的组件，比如 layer normalization 和 FFN 等。\n",
    "    input_seq = tokenized_data[None, : t + 1]\n",
    "    subsequence_probs = predict_fn(input_seq)\n",
    "    last_token_probs = subsequence_probs[0, -1]\n",
    "    log_probs.append(last_token_probs)\n",
    "  log_probs = np.vstack(log_probs)\n",
    "else:\n",
    "  log_probs = predict_fn(tokenized_data[None])[0, ...]\n",
    "probs = np.exp(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "53363f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7168\n",
      "Compression ratio: 0.4375\n"
     ]
    }
   ],
   "source": [
    "from language_modeling_is_compression import utils\n",
    "from language_modeling_is_compression import arithmetic_coder\n",
    "\n",
    "output = list()\n",
    "encoder = arithmetic_coder.Encoder(\n",
    "    base=constants.ARITHMETIC_CODER_BASE,\n",
    "    precision=constants.ARITHMETIC_CODER_PRECISION,\n",
    "    output_fn=output.append,\n",
    ")\n",
    "\n",
    "for pdf, symbol in zip(probs, tokenized_data):\n",
    "  encoder.encode(utils.normalize_pdf_for_arithmetic_coding(pdf), symbol)\n",
    "\n",
    "encoder.terminate()\n",
    "compressed_bits = ''.join(map(str, output))\n",
    "print(len(compressed_bits)) # 压缩后的bits 比如 8519 bits\n",
    "# 而因為 8519 不是 8 的倍數，所以需要 padding，最少需要在左邊pad一個bit-0\n",
    "compressed_data, num_padded_bits = utils.bits_to_bytes(compressed_bits)\n",
    "compression_ratio = len(compressed_data) / len(rawdata)\n",
    "print(f'Compression ratio: {compression_ratio:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "006ce751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterator\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "data_iter = iter(utils.bytes_to_bits(compressed_data, num_padded_bits=num_padded_bits))\n",
    "# The decoder requires a function that reads digits from {0, 1, ..., base - 1}\n",
    "# from the compressed input and returns `None` when the input is exhausted.\n",
    "def _input_fn(bit_sequence: Iterator[str] = data_iter) -> Union[int, None]:\n",
    "  try:\n",
    "    return int(next(bit_sequence))\n",
    "  except StopIteration:\n",
    "    return None\n",
    "\n",
    "decoder = arithmetic_coder.Decoder(\n",
    "    base=constants.ARITHMETIC_CODER_BASE,\n",
    "    precision=constants.ARITHMETIC_CODER_PRECISION,\n",
    "    input_fn=_input_fn,\n",
    ")\n",
    "\n",
    "# We need a dummy token because the language model right-shifts the sequence\n",
    "# by one when computing the conditional probabilities. Concretely, at every\n",
    "# step, we need the `pdf` of the next token given all currently decompressed\n",
    "# tokens, but without a dummy token, the last `pdf` would be that of the last\n",
    "# already decompressed token. The value of the dummy token is irrelevant.\n",
    "sequence_array = np.empty((1,), dtype=np.uint8)\n",
    "probs = np.exp(predict_fn(sequence_array[None])[0, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6afc8f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncompressed_length = constants.CHUNK_SIZE_BYTES\n",
    "for idx in range(uncompressed_length):\n",
    "  token = decoder.decode(\n",
    "      utils.normalize_pdf_for_arithmetic_coding(probs[idx])\n",
    "  )\n",
    "  sequence_array = np.insert(sequence_array, -1, token)\n",
    "  if len(sequence_array) == constants.CHUNK_SIZE_BYTES+1:\n",
    "    break\n",
    "  probs = np.exp(predict_fn(sequence_array[None])[0, ...])  \n",
    "decoded_data = sequence_array[:-1].tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "51336195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: Data was successfully compressed and decompressed!\n"
     ]
    }
   ],
   "source": [
    "if rawdata == decoded_data:\n",
    "  print('SUCCESS: Data was successfully compressed and decompressed!')\n",
    "else:\n",
    "  print('ERROR: Decompressed data does not match original data!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
